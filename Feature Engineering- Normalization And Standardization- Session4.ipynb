{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability Ratio Encoding\n",
    "  1. Probability of Survived based on Cabin--- Categorical Feature\n",
    "  2. Probability of Not Survived---1-pr(Survived)\n",
    "  3. pr(Survived)/pr(Not Survived)\n",
    "  4. Dictonary to map cabin with probability\n",
    "  5. with the categorical feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('titanic.csv',usecols=['Cabin','Survived'])\n",
    "df.head()\n",
    "\n",
    "Out[2]:\n",
    "Survived\tCabin\n",
    "0\t0\tNaN\n",
    "1\t1\tC85\n",
    "2\t1\tNaN\n",
    "3\t1\tC123\n",
    "4\t0\tNaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Replacing\n",
    "df['Cabin'].fillna('Missing',inplace=True)\n",
    "df.head()\n",
    "\n",
    "Out[3]:\n",
    "Survived\tCabin\n",
    "0\t0\tMissing\n",
    "1\t1\tC85\n",
    "2\t1\tMissing\n",
    "3\t1\tC123\n",
    "4\t0\tMissing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cabin'].unique()\n",
    "\n",
    "Out[5]:\n",
    "array(['Missing', 'C85', 'C123', 'E46', 'G6', 'C103', 'D56', 'A6',\n",
    "       'C23 C25 C27', 'B78', 'D33', 'B30', 'C52', 'B28', 'C83', 'F33',\n",
    "       'F G73', 'E31', 'A5', 'D10 D12', 'D26', 'C110', 'B58 B60', 'E101',\n",
    "       'F E69', 'D47', 'B86', 'F2', 'C2', 'E33', 'B19', 'A7', 'C49', 'F4',\n",
    "       'A32', 'B4', 'B80', 'A31', 'D36', 'D15', 'C93', 'C78', 'D35',\n",
    "       'C87', 'B77', 'E67', 'B94', 'C125', 'C99', 'C118', 'D7', 'A19',\n",
    "       'B49', 'D', 'C22 C26', 'C106', 'C65', 'E36', 'C54',\n",
    "       'B57 B59 B63 B66', 'C7', 'E34', 'C32', 'B18', 'C124', 'C91', 'E40',\n",
    "       'T', 'C128', 'D37', 'B35', 'E50', 'C82', 'B96 B98', 'E10', 'E44',\n",
    "       'A34', 'C104', 'C111', 'C92', 'E38', 'D21', 'E12', 'E63', 'A14',\n",
    "       'B37', 'C30', 'D20', 'B79', 'E25', 'D46', 'B73', 'C95', 'B38',\n",
    "       'B39', 'B22', 'C86', 'C70', 'A16', 'C101', 'C68', 'A10', 'E68',\n",
    "       'B41', 'A20', 'D19', 'D50', 'D9', 'A23', 'B50', 'A26', 'D48',\n",
    "       'E58', 'C126', 'B71', 'B51 B53 B55', 'D49', 'B5', 'B20', 'F G63',\n",
    "       'C62 C64', 'E24', 'C90', 'C45', 'E8', 'B101', 'D45', 'C46', 'D30',\n",
    "       'E121', 'D11', 'E77', 'F38', 'B3', 'D6', 'B82 B84', 'D17', 'A36',\n",
    "       'B102', 'B69', 'E49', 'C47', 'D28', 'E17', 'A24', 'C50', 'B42',\n",
    "       'C148'], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cabin']=df['Cabin'].astype(str).str[0]\n",
    "df.head()\n",
    "\n",
    "\n",
    "Out[6]:\n",
    "Survived\tCabin\n",
    "0\t0\tM\n",
    "1\t1\tC\n",
    "2\t1\tM\n",
    "3\t1\tC\n",
    "4\t0\tM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Cabin.unique()\n",
    "\n",
    "\n",
    "Out[7]:\n",
    "array(['M', 'C', 'E', 'G', 'D', 'A', 'B', 'F', 'T'], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df=df.groupby(['Cabin'])['Survived'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df=pd.DataFrame(prob_df)\n",
    "prob_df\n",
    "\n",
    "\n",
    "Out[10]:\n",
    "Survived\n",
    "Cabin\t\n",
    "A\t0.466667\n",
    "B\t0.744681\n",
    "C\t0.593220\n",
    "D\t0.757576\n",
    "E\t0.750000\n",
    "F\t0.615385\n",
    "G\t0.500000\n",
    "M\t0.299854\n",
    "T\t0.000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df['Died']=1-prob_df['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df.head()\n",
    "\n",
    "\n",
    "Out[12]:\n",
    "Survived\tDied\n",
    "Cabin\t\t\n",
    "A\t0.466667\t0.533333\n",
    "B\t0.744681\t0.255319\n",
    "C\t0.593220\t0.406780\n",
    "D\t0.757576\t0.242424\n",
    "E\t0.750000\t0.250000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df['Probability_ratio']=prob_df['Survived']/prob_df['Died']\n",
    "prob_df.head()\n",
    "\n",
    "\n",
    "Out[13]:\n",
    "Survived\tDied\tProbability_ratio\n",
    "Cabin\t\t\t\n",
    "A\t0.466667\t0.533333\t0.875000\n",
    "B\t0.744681\t0.255319\t2.916667\n",
    "C\t0.593220\t0.406780\t1.458333\n",
    "D\t0.757576\t0.242424\t3.125000\n",
    "E\t0.750000\t0.250000\t3.000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_encoded=prob_df['Probability_ratio'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cabin_encoded']=df['Cabin'].map(probability_encoded)\n",
    "df.head()\n",
    "\n",
    "\n",
    "Out[16]:\n",
    "Survived\tCabin\tCabin_encoded\n",
    "0\t0\tM\t0.428274\n",
    "1\t1\tC\t1.458333\n",
    "2\t1\tM\t0.428274\n",
    "3\t1\tC\t1.458333\n",
    "4\t0\tM\t0.428274"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(20)\n",
    "\n",
    "\n",
    "Out[17]:\n",
    "Survived\tCabin\tCabin_encoded\n",
    "0\t0\tM\t0.428274\n",
    "1\t1\tC\t1.458333\n",
    "2\t1\tM\t0.428274\n",
    "3\t1\tC\t1.458333\n",
    "4\t0\tM\t0.428274\n",
    "5\t0\tM\t0.428274\n",
    "6\t0\tE\t3.000000\n",
    "7\t0\tM\t0.428274\n",
    "8\t1\tM\t0.428274\n",
    "9\t1\tM\t0.428274\n",
    "10\t1\tG\t1.000000\n",
    "11\t1\tC\t1.458333\n",
    "12\t0\tM\t0.428274\n",
    "13\t0\tM\t0.428274\n",
    "14\t0\tM\t0.428274\n",
    "15\t1\tM\t0.428274\n",
    "16\t0\tM\t0.428274\n",
    "17\t1\tM\t0.428274\n",
    "18\t0\tM\t0.428274\n",
    "19\t1\tM\t0.428274"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation of Features\n",
    "Why Transformation of Features Are Required?\n",
    "\n",
    "   1.Linear Regression---Gradient Descent ----Global Minima\n",
    "   2.Algorithms like KNN,K Means,Hierarichal Clustering--- Eucledian Distance\n",
    "Every Point has some vectors and Directiom\n",
    "\n",
    "Deep Learning Techniques(Standardization, Scaling) 1.ANN--->GLobal Minima, Gradient 2.CNN 3.RNN\n",
    "\n",
    "0-255 pixels\n",
    "\n",
    "Types Of Transformation\n",
    "  1.Normalization And Standardization\n",
    "  2.Scaling to Minimum And Maximum values\n",
    "  3.Scaling To Median And Quantiles\n",
    "  4.Guassian Transformation Logarithmic Transformation Reciprocal Trnasformation Square Root \n",
    "Transformation Exponential Trnasformation Box Cox Transformation\n",
    "Standardization\n",
    "We try to bring all the variables or features to a similar scale. \n",
    "standarisation means centering the variable at zero. z=(x-x_mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('titanic.csv', usecols=['Pclass','Age','Fare','Survived'])\n",
    "df.head()\n",
    "\n",
    "\n",
    "Out[1]:\n",
    "Survived\tPclass\tAge\tFare\n",
    "0\t0\t3\t22.0\t7.2500\n",
    "1\t1\t1\t38.0\t71.2833\n",
    "2\t1\t3\t26.0\t7.9250\n",
    "3\t1\t1\t35.0\t53.1000\n",
    "4\t0\t3\t35.0\t8.0500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age'].fillna(df.Age.median(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()\n",
    "\n",
    "\n",
    "Out[3]:\n",
    "Survived    0\n",
    "Pclass      0\n",
    "Age         0\n",
    "Fare        0\n",
    "dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### standarisation: We use the Standardscaler from sklearn library\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=StandardScaler()\n",
    "### fit vs fit_transform\n",
    "df_scaled=scaler.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled\n",
    "\n",
    "\n",
    "Out[12]:\n",
    "array([[-0.78927234,  0.82737724, -0.56573646, -0.50244517],\n",
    "       [ 1.2669898 , -1.56610693,  0.66386103,  0.78684529],\n",
    "       [ 1.2669898 ,  0.82737724, -0.25833709, -0.48885426],\n",
    "       ...,\n",
    "       [-0.78927234,  0.82737724, -0.1046374 , -0.17626324],\n",
    "       [ 1.2669898 , -1.56610693, -0.25833709, -0.04438104],\n",
    "       [-0.78927234,  0.82737724,  0.20276197, -0.49237783]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_scaled[:,1],bins=20)\n",
    "\n",
    "\n",
    "Out[11]:\n",
    "(array([216.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 184.,\n",
    "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 491.]),\n",
    " array([-1.56610693, -1.44643272, -1.32675851, -1.2070843 , -1.08741009,\n",
    "        -0.96773588, -0.84806167, -0.72838747, -0.60871326, -0.48903905,\n",
    "        -0.36936484, -0.24969063, -0.13001642, -0.01034222,  0.10933199,\n",
    "         0.2290062 ,  0.34868041,  0.46835462,  0.58802883,  0.70770304,\n",
    "         0.82737724]),\n",
    " <a list of 20 Patch objects>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_scaled[:,2],bins=20)\n",
    "\n",
    "\n",
    "Out[13]:\n",
    "(array([ 40.,  14.,  15.,  31.,  79.,  98., 262.,  84.,  73.,  45.,  35.,\n",
    "         35.,  29.,  16.,  13.,  11.,   4.,   5.,   1.,   1.]),\n",
    " array([-2.22415608, -1.91837055, -1.61258503, -1.3067995 , -1.00101397,\n",
    "        -0.69522845, -0.38944292, -0.08365739,  0.22212813,  0.52791366,\n",
    "         0.83369919,  1.13948471,  1.44527024,  1.75105577,  2.05684129,\n",
    "         2.36262682,  2.66841235,  2.97419787,  3.2799834 ,  3.58576892,\n",
    "         3.89155445]),\n",
    " <a list of 20 Patch objects>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt.hist(df_scaled[:,3],bins=20)\n",
    "\n",
    "\n",
    "Out[14]:\n",
    "(array([562., 170.,  67.,  39.,  15.,  16.,   2.,   0.,   9.,   2.,   6.,\n",
    "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   3.]),\n",
    " array([-0.64842165, -0.13264224,  0.38313716,  0.89891657,  1.41469598,\n",
    "         1.93047539,  2.4462548 ,  2.96203421,  3.47781362,  3.99359303,\n",
    "         4.50937244,  5.02515184,  5.54093125,  6.05671066,  6.57249007,\n",
    "         7.08826948,  7.60404889,  8.1198283 ,  8.63560771,  9.15138712,\n",
    "         9.66716653]),\n",
    " <a list of 20 Patch objects>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(df['Fare'],bins=20)\n",
    "\n",
    "Out[15]:\n",
    "(array([562., 170.,  67.,  39.,  15.,  16.,   2.,   0.,   9.,   2.,   6.,\n",
    "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   3.]),\n",
    " array([  0.     ,  25.61646,  51.23292,  76.84938, 102.46584, 128.0823 ,\n",
    "        153.69876, 179.31522, 204.93168, 230.54814, 256.1646 , 281.78106,\n",
    "        307.39752, 333.01398, 358.63044, 384.2469 , 409.86336, 435.47982,\n",
    "        461.09628, 486.71274, 512.3292 ]),\n",
    " <a list of 20 Patch objects>)\n",
    "\n",
    "\n",
    "Min Max Scaling (### CNN)---Deep Learning Techniques\n",
    "Min Max Scaling scales the values between 0 to 1. X_scaled = (X - X.min / (X.max - X.min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "min_max=MinMaxScaler()\n",
    "df_minmax=pd.DataFrame(min_max.fit_transform(df),columns=df.columns)\n",
    "df_minmax.head()\n",
    "\n",
    "\n",
    "Out[17]:\n",
    "Survived\tPclass\tAge\tFare\n",
    "0\t0.0\t1.0\t0.271174\t0.014151\n",
    "1\t1.0\t0.0\t0.472229\t0.139136\n",
    "2\t1.0\t1.0\t0.321438\t0.015469\n",
    "3\t1.0\t0.0\t0.434531\t0.103644\n",
    "4\t0.0\t1.0\t0.434531\t0.015713"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_minmax['Pclass'],bins=20)\n",
    "\n",
    "\n",
    "Out[18]:\n",
    "(array([216.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 184.,\n",
    "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 491.]),\n",
    " array([0.  , 0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 ,\n",
    "        0.55, 0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95, 1.  ]),\n",
    " <a list of 20 Patch objects>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_minmax['Age'],bins=20)\n",
    "\n",
    "\n",
    "Out[19]:\n",
    "(array([ 40.,  14.,  15.,  31.,  79.,  98., 262.,  84.,  73.,  45.,  35.,\n",
    "         35.,  29.,  16.,  13.,  11.,   4.,   5.,   1.,   1.]),\n",
    " array([0.  , 0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 ,\n",
    "        0.55, 0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95, 1.  ]),\n",
    " <a list of 20 Patch objects>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt.hist(df_minmax['Fare'],bins=20)\n",
    "\n",
    "\n",
    "Out[20]:\n",
    "(array([562., 170.,  67.,  39.,  15.,  16.,   2.,   0.,   9.,   2.,   6.,\n",
    "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   3.]),\n",
    " array([0.  , 0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 ,\n",
    "        0.55, 0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95, 1.  ]),\n",
    " <a list of 20 Patch objects>)\n",
    "\n",
    "Robust Scaler\n",
    "It is used to scale the feature to median and quantiles Scaling using median and quantiles consists of substracting the median to all the observations, and then dividing by the interquantile difference. The interquantile difference is the difference between the 75th and 25th quantile:\n",
    "\n",
    "IQR = 75th quantile - 25th quantile\n",
    "\n",
    "X_scaled = (X - X.median) / IQR\n",
    "\n",
    "0,1,2,3,4,5,6,7,8,9,10\n",
    "\n",
    "9-90 percentile---90% of all values in this group is less than 9 1-10 precentile\n",
    "---10% of all values in this group is less than 1 4-40%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler=RobustScaler()\n",
    "df_robust_scaler=pd.DataFrame(scaler.fit_transform(df),columns=df.columns)\n",
    "df_robust_scaler.head()\n",
    "\n",
    "Out[21]:\n",
    "Survived\tPclass\tAge\tFare\n",
    "0\t0.0\t0.0\t-0.461538\t-0.312011\n",
    "1\t1.0\t-2.0\t0.769231\t2.461242\n",
    "2\t1.0\t0.0\t-0.153846\t-0.282777\n",
    "3\t1.0\t-2.0\t0.538462\t1.673732\n",
    "4\t0.0\t0.0\t0.538462\t-0.277363"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_robust_scaler['Age'],bins=20)\n",
    "\n",
    "\n",
    "Out[23]:\n",
    "(array([ 40.,  14.,  15.,  31.,  79.,  98., 262.,  84.,  73.,  45.,  35.,\n",
    "         35.,  29.,  16.,  13.,  11.,   4.,   5.,   1.,   1.]),\n",
    " array([-2.12153846, -1.81546154, -1.50938462, -1.20330769, -0.89723077,\n",
    "        -0.59115385, -0.28507692,  0.021     ,  0.32707692,  0.63315385,\n",
    "         0.93923077,  1.24530769,  1.55138462,  1.85746154,  2.16353846,\n",
    "         2.46961538,  2.77569231,  3.08176923,  3.38784615,  3.69392308,\n",
    "         4.        ]),\n",
    " <a list of 20 Patch objects>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_robust_scaler['Fare'],bins=20)\n",
    "\n",
    "\n",
    "Out[24]:\n",
    "(array([562., 170.,  67.,  39.,  15.,  16.,   2.,   0.,   9.,   2.,   6.,\n",
    "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   3.]),\n",
    " array([-0.62600478,  0.48343237,  1.59286952,  2.70230667,  3.81174382,\n",
    "         4.92118096,  6.03061811,  7.14005526,  8.24949241,  9.35892956,\n",
    "        10.46836671, 11.57780386, 12.68724101, 13.79667816, 14.90611531,\n",
    "        16.01555246, 17.12498961, 18.23442675, 19.3438639 , 20.45330105,\n",
    "        21.5627382 ]),\n",
    " <a list of 20 Patch objects>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guassian Transformation\n",
    "Some machine learning algorithms like linear and logistic assume that the features are normally distributed -Accuracy -Performance\n",
    "\n",
    "  1.logarithmic transformation\n",
    "  2.reciprocal transformation\n",
    "  3.square root transformation\n",
    "  4.exponential transformation (more general, you can use any exponent)\n",
    "  5.boxcox transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=pd.read_csv('titanic.csv',usecols=['Age','Fare','Survived'])\n",
    "df.head()\n",
    "\n",
    "\n",
    "Out[25]:\n",
    "Survived\tAge\tFare\n",
    "0\t0\t22.0\t7.2500\n",
    "1\t1\t38.0\t71.2833\n",
    "2\t1\t26.0\t7.9250\n",
    "3\t1\t35.0\t53.1000\n",
    "4\t0\t35.0\t8.0500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### fillnan\n",
    "df['Age']=df['Age'].fillna(df['Age'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()\n",
    "\n",
    "\n",
    "Out[29]:\n",
    "Survived    0\n",
    "Age         0\n",
    "Fare        0\n",
    "dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stat\n",
    "import pylab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### If you want to check whether feature is guassian or normal distributed\n",
    "#### Q-Q plot\n",
    "def plot_data(df,feature):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.subplot(1,2,1)\n",
    "    df[feature].hist()\n",
    "    plt.subplot(1,2,2)\n",
    "    stat.probplot(df[feature],dist='norm',plot=pylab)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(df,'Age')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logarithmic Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df['Age_log']=np.log(df['Age'])\n",
    "plot_data(df,'Age_log')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reciprocal Trnasformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age_reciprocal']=1/df.Age\n",
    "plot_data(df,'Age_reciprocal')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Square Root Transformation\n",
    "df['Age_sqaure']=df.Age**(1/2)\n",
    "plot_data(df,'Age_sqaure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Exponential Transdormation\n",
    "df['Age_exponential']=df.Age**(1/1.2)\n",
    "plot_data(df,'Age_exponential')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoxCOx Transformation\n",
    "The Box-Cox transformation is defined as:\n",
    "\n",
    "T(Y)=(Y exp(λ)−1)/λ\n",
    "\n",
    "where Y is the response variable and λ is the transformation parameter. \n",
    "λ varies from -5 to 5. In the transformation, \n",
    "all values of λ are considered and the optimal value for a given variable is selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age_Boxcox'],parameters=stat.boxcox(df['Age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parameters)\n",
    "0.7964531473656952"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(df,'Age_Boxcox')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(df,'Fare')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Fare\n",
    "df['Fare_log']=np.log1p(df['Fare'])\n",
    "plot_data(df,'Fare_log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Fare_Boxcox'],parameters=stat.boxcox(df['Fare']+1)\n",
    "plot_data(df,'Fare_Boxcox')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
